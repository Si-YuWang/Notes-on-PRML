\documentclass[a4paper]{book}

\usepackage{amsmath,longtable,fancyhdr,booktabs,multirow,graphicx,float}
\usepackage{amssymb}
\usepackage{color}
\usepackage[colorlinks,
            linkcolor=black,
            anchorcolor=blue,
            citecolor=green
           ]{hyperref}
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{CJKnumb,titlesec,titletoc}
\usepackage{mnsymbol}
\usepackage{theorem}
\usepackage{algorithmicx}

\usepackage[nottoc]{tocbibind}
\def\ci{\perp\!\!\!\perp}
\pagestyle{fancy}
%% define some commands
\newcommand{\ud}{\mathrm{d}}
\newcommand{\e}{\varepsilon}
\newcommand{\up}{\mathrm}
\def\dbar{\mathrm{\mathchar'26\mkern-12mu d}}
\newcommand{\wave}{\scriptsize{\sim}}
\renewcommand{\bf}{\mathbf}
\renewcommand{\cal}{\mathcal}
\newcommand{\mbf}{\mathbf}
\newcommand{\mcal}{\mathcal}
\newcommand{\bb}{\mathbb}
\newcommand{\mbb}{\mathbb}
\newcommand{\imp}[1]{\textit{#1}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mbs}{\boldsymbol}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\def\dbar{\mathrm{\mathchar'26\mkern-12mu d}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Thm\onedot~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\propref}[1]{Prop\onedot~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}
\newcommand{\yangs}[1]{{\color{cyan}{\bf\sf [YS: #1]}}}
\newcommand{\yang}[1]{{\color{cyan}{\bf\sf [#1]}}}
\newcommand{\Ep}{\mcal{E}}
\newcommand{\Epp}{\widehat{\mcal{E}}_s^+[\mu]}
\newcommand{\Eln}{\widehat{\mcal{E}}_{\lambda,n}[\mu]}
\newcommand{\mup}{\widehat{\mu}_{(X,Y)}^+}
\newcommand{\muln}{\widehat{\mu}_{\lambda,n}}
\newcommand{\Hy}{\mcal{H}_{\mcal{Y}}}
\newcommand{\Hx}{\mcal{H}_{\mcal{X}}}
\newcommand{\HK}{\mcal{H}_{K}}
\newcommand{\HH}{\mcal{H}}
\newcommand{\XX}{\mcal{X}}
\newcommand{\YY}{\mcal{Y}}
\newcommand{\ZZ}{\mcal{Z}}
\newcommand{\BB}{\mcal{B}}
\newcommand{\zz}{\mbf{z}}
\newcommand{\xx}{\mbf{x}}
\newcommand{\yy}{\mbf{y}}
\newcommand{\CC}{\mcal{C}}
\newcommand{\hC}{\widehat{\mcal{C}}}
\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot}
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\aka{a.k.a\onedot}
\def\iid{i.i.d\onedot}
\def\etal{\emph{et al}\onedot}

\title{Notes of PRML}
\author{Siyu Wang}
\date{August 2018}

\begin{document}

\maketitle
\chapter{Mathematical Foundations}

\chapter{Probabilistic Models}
\section{Graphical Models}
We shall find it highly advantageous to augment the analysis using diagrammatic representations of probability distributions, called \textit{probabilistic graphical models}. These offer several useful properties:
\begin{enumerate}
    \item They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.
    \item Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph.
    \item Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.
\end{enumerate}
\textit{nodes}: random variables. \textit{links}: probabilistic relationships between the variables.
\newline
\textit{Bayesan networks (directed graphical models)}, \textit{undirected graphical models}, \textit{factor graph}.

\subsection{Bayesian Networks}
Decomposition of a joint probability distribution:
\begin{equation}
    \tag{8.1} p(a, b, c) = p(c|a, b)p(b|a)p(a)
    \label{eq8.1}
\end{equation}
and the corresponding graphical model can be seen in figure\ref{GM1}.
\begin{figure}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\linewidth]{./imgs/GM1.eps}
    \caption{}
    \label{GM1}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\linewidth]{./imgs/GM2.eps}
    \caption{}
    \label{GM2}
\end{minipage}
\end{figure}
\newline
\textbf{Rule}: for each conditional distribution we add directed links (arrows) to the graph from the nodes corresponding to the variables on which the distribution is conditioned.
\newline
we can extend the decomposition to $K$ variables:
\begin{equation}
    p(x_1,...,x_K) = p(x_K|x_1,...,x_{K-1})...p(x_2|x_1)p(x_1).
    \label{eq8.2}
\end{equation}
And the corresponding directed graph is called \textit{fully connected} because there is a link between every pair of nodes.
\newline
\textbf{Case} Given a directed graph as seen in \ref{GM2}, we can write the corresponding probability product:
\begin{equation}
    p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_2,x_3)p(x_6|x_4)p(x_7|x_4,x_5)
    \label{8.3}
\end{equation}
\textbf{Rule}:
\begin{equation}
p(\textbf{x}) = \Pi_{k=1}^Kp(x_k|\mathrm{pa}_k)
\label{8.4}
\end{equation}
where $\mathrm{pa}_k$ denotes the set of parents of $x_k$, and $\textbf{x} = \{x_1,..,x_K\}$.
\newline
The directed graphs that we are considering are subject to an important restriction namely that there must be no \textit{directed cycles}, in other words there are no closed paths within the graph such that we can move from node to node along links following the direction of the arrows and end up back at the starting node. Such graphs are also called \textit{directed acyclic graphs}, or \textit{DAGs}. This is equivalent to the statement that there exists an ordering of the nodes such that there are no links that go from any node to any lower numbered node.
\subsubsection{Example: polynomial regression}
For more complex models, we shall adopt the convention that random variables will be denoted by open circles, and deterministic parameters will be denoted by smaller solid circles.
some concepts: \textit{observed variables, hidden variables, deterministic parameters}.
\subsubsection{Generative models}
\textit{sampling}: given a joint distribution, we want to draw a sample $\hat x_1, \hat x_2, ..., \hat x_K$ from it.
\textbf{ancestral sampling}
\newline
We start with the lowest-numbered node and draw a sample from the distribution $p(x_1)$, which we all $\hat x_1$. Then we work through each of the nodes in order, so that for node n we draw a sample from the conditional distribution p(xn|pan)
in which the parent variables have been set to their sampled values. Note that at each stage, these parent values will always be available because they correspond to lower-numbered
nodes that have already been sampled. The graphical model captures the \textit{causal} process by which the observed data was generated. For this reason, such models are often called \textit{generative} models. But the regression problem is not generative because there is no probability distribution associated with the input variable $x$, and it is not possible to generate synthetic data points from this model. But we can make it generative by introducing a suitable prior distribution $p(x)$, at the expense of a more complex model.
\subsubsection{Discrete variables}
\textit{parent-child pair in a directed graph. Two cases are particularly worthy of note, namely when the parent and child node each correspond to discrete variables and when they each correspond to Gaussian variables, because in these two cases the relationship can be extended hierarchically to construct arbitrarily complex directed acyclic graphs.}
\newline
\textbf{Case}:  discrete variable $\textbf{x}$ having $K$ possible states is given by
\begin{equation}\label{eq8.5}
  p(\bf{x}|\bf{mu}) =  \Pi_{k=1}^K\mu_k^{x_k}
\end{equation} governed by $\bf{\mu}$.
Suppose we have to discrete variables $\bf x_1, \bf x_2$, joint distribution can be written as
\begin{equation}\label{eq8.6}
  p(\bf x_1, \bf x_2|\bf{\mu})=\Pi_{k=1}^K\Pi_{k=1}^K\mu_{kl}^{x_{1k}x_{2l}}.
\end{equation}
$x_{1k}$ denotes the $k^{th}$ component of $\bf x_1$.
\newline
$K^2-1$ parameters. If we have $M$ variables, there is $K^M-1$ parameters, exponential with the num $M$.
\newline
Suppose $\bf x_1$ and $\bf x_2$ are independent. Each variable is then described by a separate multinomial distribution, and total number of parameters will be $2(K-1)$. If extend to $M$ variables, there will be $M(K-1)$ variables, linear growth.
\newline
\textbf{Generally case}: more links than independent variables but less links than a fully connected graph.
\newline
We can turn a graph over discrete variables into a Bayesian model by introducing Dirichlet priors for the parameters.
\newline
Another way of controlling the exponential growth in the number of parameters
in models of discrete variables is to use parameterized models for the conditional
distributions instead of complete tables of conditional probability values.
\subsubsection{Linear-Gaussian models}

\subsection{Conditional Independence}
If
\begin{equation}\label{eq8.7}
  p(a|b, c) = p(a|c),
\end{equation}
we say that $a$ is conditionally independent of $b$ given $c$, and we denote this by $$a\ci b|c .$$  And we will have
\begin{equation}\label{eq8.8}
  p(a,b|c) = p(a|c)p(b|c).
\end{equation}
\subsubsection{Examples}
Three examples related to conditional independence: \textit{tail-to-tail, head-to-tail, head-to-head}.
\subsubsection{D-separation}
We wish to ascertain whether a particular conditional
independence statement$$A\ci B |C$$is implied by a given directed acyclic graph.
\newline
i.i.d data points. Given $\mu$, we can say the data points $x_1,x_2,...,x_N$ are conditionally independent, but we can not say the data points are independent. because given $x_1$, the probability of $\mu$ will be affected and then $x_2$ will be affected.
\newline
\textbf{naive Bayes Model} Observation of $\bf{z}$ will block the path between $x_i$ and $x_j$ for $j\neq i$. So, if we are given a training set will, comprising inputs $\{x_1,...,x_N\}$ together with their labels, then we can fit the naive Bayes model to the training data using maximum likelihood assuming that the data are drawn independently from the model.
\newline
We can view a directed graph as a filter and all probability distribution $p(\bf x)$ that will be allowed through can make a set $\mathcal {DF}$.
\newline
\textbf{Markov blanket}.
\begin{equation}\label{eq8.9}
  p(\bf{x}_i|\bf{x}_{\{j\neq i\}}) = \frac{p(\bf x_1,...\bf x_D)}{\int p(\bf x_1,...,\bf x_D)\rm d\bf x_i}=\frac{\prod_k p(\bf x_k|\rm{pa}_k)}{\int \prod_k p(\bf x_k|\rm{pa}_k)\rm{d}\bf x_i}
\end{equation}
Some factors in $\prod_k p(\bf x_k|\rm{pa}_k)$ will disappear if they are not relative to $\bf x_i$.
\subsection{Markov Random Fields}
A \emph{Markov random field}, also known as a \emph{Markov network} or an \emph{undirected graphical model} , has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes. The links are undirected, that is they do not carry arrows.
\subsubsection{Conditional independence properties}
Properties in an undirected graph is much simpler than those in a directed graph because the nodes linked in a pair is symmetric, and there is no head-to-head situation. So if all paths that connect nodes in $A$ to nodes in $B$ pass through one or more nodes in the set $C$, then all such paths are 'blocked' and so the conditional independence property hold, aka. $$A\ci B|C$$.
\subsubsection{Factorization properties}
If two nodes are not linked directly, then we can obtain
\begin{equation}\label{eq8.10}
  p(x_i,x_j|\bf{x}_{\backslash\{i,j\}}) = p(x_i|\bf x_{\backslash\{i,j\}})p(x_j|\bf x_{\backslash\{i,j\}})
\end{equation}
\textbf{clique}:a subset of nodes in a graph such that there exists a link between all pairs of nodes in the subset. \newline
\textbf{maximal clique}:a clique such that it is not possible to include any
other nodes from the graph in the set without it ceasing to be a clique.\newline
Denote a clique by $C$ and the set of variables in that clique by $\bf x_C$. Then the joint distribution is written as a product of \emph{potential functions}
$\phi_C(\bf x_C)$ over maximal cliques of the graph
\begin{equation}\label{eq8.11}
  p(\bf x) = \frac{1}{Z}\prod_{C}\phi_C(\bf x_C).
\end{equation}
Here the quantity Z, sometimes called the partition function, is a normalization constant
and is given by
\begin{equation}\label{eq8.12}
  Z=\sum_{X}\prod_{C}\phi_C(\bf x_C)
\end{equation}
\emph{energy function} $E(\bf x_C)$
\begin{equation}\label{eq8.13}
  \phi_C(\bf x_C) = \exp\{-E(\bf x_C\}
\end{equation}
\subsubsection{Relation to directed graphs}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./imgs/GM3.eps}
  \caption{}\label{GM3}
\end{figure}
Given a directed graph as a chain shown in figure\ref{GM3}(a).
\begin{equation}\label{eq8.14}
  p(\bf x) =p(x_1)p(x_2|x_1)p(x_3|x_2)...p(x_N|x_{N-1})
\end{equation}
Also, for undirected graph shown in figure\ref{GM3}(b).
\begin{equation}\label{eq8.15}
  p(\bf x) = \frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)...\psi_{N-1,N}(x_{N-1},x_N).
\end{equation}
And we define
\begin{align}\label{eq8.15}
  \psi_{1,2}(x_1,x_2) = p(x_1)p(x_2|x_1) \\
  \psi_{2,3}(x_2,x_3)=p(x_3|x_2) \\
  \vdots \\
  \psi_{N-1,N}(x_{N-1},x_N) = p(x_N|x_{N-1})
\end{align}

\chapter{Algorithms}
\section{Inference Algorithms in Graphical Models}
\subsection{Inference on a chain}
Consider the graph shown in figure\ref{GM3}. The joint distribution for this graph takes the form
\begin{equation}\label{eq8.16}
  p(\bf x) = \frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)...\psi_{N-1,N}(x_{N-1},x_N).
\end{equation}
If we want to get marginal distribution $p(x_n)$ for a specific node, we should sum the joint distribution over all variables except $x_n$, so that
\begin{equation}\label{eq8.17}
  p(x_n) = \sum_{x_1}...\sum_{x_{n-1}}\sum_{x_{n+1}}...\sum_{x_N}p(\bf x)
\end{equation}
But this way will cost so much computational resources, so we want to find another simpler way:
\begin{align}\label{eq8.18}
  p(x_n) =  & \frac{1}{Z}[\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\dots[\sum_{x_2}\psi_{2,3}(x_2,x_3)[\sum_{x_1}\psi_{1,2}(x_1,x_2)]]\dots] \\
   & [\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\dots[\sum_{x_N}]\psi_{N-1,N}(x_{N-1},x_{N})]\dots]  \\
   = & \frac{1}{Z}\mu_{\alpha}(x_n)\mu_{\beta}(x_n)
\end{align}
As shown in figure\ref{GM4}, we can see message passed in the chain and we can compute $\mu_{\alpha}(x_i)$ and $\mu_{\beta}(x_i)$ recursively for any $i=1, 2, \dots, N$.
\begin{equation}\label{eq8.19}
  \mu_{\alpha}(x_n) = \sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\mu_{\alpha}(x_{n-1}).
\end{equation}
and
\begin{equation}\label{eq8.20}
  \mu_{\alpha}(x_2) = \sum_{x_1}\psi_{1,2}(x_1,x_2).
\end{equation}
Similar for $\mu_{\beta}(x_n)$
\begin{figure}[bth]
  \centering
  \includegraphics[width=\textwidth]{./imgs/GM4.eps}
  \caption{}\label{GM4}
\end{figure}
\subsection{Inference on a tree: sum-product algorithm}
Tree do not have loops.
\subsubsection{factor graphs}
In a factor graph, there is a node (depicted as usual by \textbf{a circle}) for every variable
in the distribution, as was the case for directed and undirected graphs. There are also
additional nodes (depicted by \textbf{small squares}) for each factor fs(xs) in the joint distribution.\newline
For example,
\begin{equation}\label{eq8.21}
  p(\bf{x}) = f_a(x_1,x_2)f_b(x_1,x_2)f_c(x_2,x_3)f_d(x_3)
\end{equation}
can be expressed by the factor graph shown in figure\ref{GM5}
\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./imgs/GM5.jpg}
  \caption{Example of a factor graph}\label{GM5}
\end{figure}
Factor graphs are said to be bipartite because they consist of two distinct kinds
of nodes, and all links go between nodes of opposite type.
\subsubsection{sum-product algorithm}
We shall now make use of the factor graph framework to derive a powerful class
of efficient, exact inference algorithms that are applicable to tree-structured graphs.
Here we shall focus on the problem of evaluating local marginals over nodes or
subsets of nodes, which will lead us to the sum-product algorithm.\newline
Our goal is to exploit the structure of
the graph to achieve two things: (i) to obtain an efficient, exact inference algorithm
for finding marginals; (ii) in situations where several marginals are required to allow
computations to be shared efficiently.
\textbf{finding the marginal $p(x)$ for a particular variable node $x$}\newline
$$p(\bf x) = \prod_{s\in ne(x)}F_s(x,X_s)   $$
$$p(x) = \prod_{s\in ne(x)}[\sum_{X_s}F_s(x,X_s)] = \prod_{s\in ne(x)}\mu_{f_s\rightarrow x}(x) $$
$$F_s(x,X_s) =  f_s(x,x_1,...x_M)G_1(x,X_{s1})...G_M(x,X_{sM})$$
$$\mu_{x_m\rightarrow f_s}(x_m) \equiv \sum_{X_{sm}}G_m(x_m,X_{sm})$$
We have therefore introduced two distinct kinds of message, those that go from factor
nodes to variable nodes denoted $\mu_{f\rightarrow x}(x)$, and those that go from variable nodes to
factor nodes denoted $\mu_{x\rightarrow f}(x).$
\textbf{Illustration: example}
\textbf{max-sum algorithm}
Two other common tasks are to find a setting of the variables that has the largest probability
and to find the value of that probability. We therefore seek an efficient algorithm for finding the value of x that maximizes
the joint distribution p(x) and that will allow us to obtain the value of the
joint distribution at its maximum.
\subsection{Exact inference in a general graph}
Goal: deal with graphs having loops.\newline
\textbf{Loopy belief progpagation}
One simple approach to approximate inference in graphs with
loops, which builds directly on the previous discussion of exact inference in trees.
The idea is simply to apply the sum-product algorithm even though there is no guarantee
that it will yield good results. For some graphs, the algorithm will converge, whereas for others it will not. \newline
We will say that a (variable or factor) node a has a message pending on its link to a node b if node a has received any
message on any of its other links since the last time it send a message to b.

\end{document} 